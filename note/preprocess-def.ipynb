{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Datasets\n",
    "\n",
    "Final Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import arff\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from os.path import dirname\n",
    "from aaai20.io import filename_dataset, filename_query, original_filename\n",
    "from aaai20.wrangling import arff_to_df\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modulo.utils.encoding import query_to_code, code_to_query\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(nb_atts, targ_idx=-1, nb_qry=10, random_state=42):\n",
    "    # init ids\n",
    "    attr_ids = list(range(nb_atts))\n",
    "    targ_ids = [attr_ids[targ_idx]] # Last attribute by default\n",
    "    desc_ids = [e for e in attr_ids if e not in targ_ids]\n",
    "    miss_ids = []\n",
    "\n",
    "    q_targ = [targ_ids]\n",
    "    q_desc = [desc_ids]\n",
    "    q_miss = [miss_ids]\n",
    "\n",
    "    # Start query buiding\n",
    "    nb_of_attributes_to_make_missing = np.linspace(0, nb_atts-1, nb_qry, endpoint=False, dtype=int)\n",
    "    nb_items_to_transfer = np.ediff1d(nb_of_attributes_to_make_missing)\n",
    "\n",
    "    for qry_id, e in enumerate(nb_items_to_transfer):\n",
    "        desc_ids, miss_ids = transfer_contents(desc_ids, miss_ids, nb_items_to_transfer=e, random_state=random_state)\n",
    "\n",
    "        #print(desc_ids, miss_ids, targ_ids)\n",
    "        q_targ.append(targ_ids)\n",
    "        q_desc.append(desc_ids)\n",
    "        q_miss.append(miss_ids)\n",
    "    \n",
    "    return q_desc, q_targ, q_miss\n",
    "\n",
    "def transfer_contents(list_one, list_two, nb_items_to_transfer=1, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    list_one, list_two = list_one.copy(), list_two.copy() \n",
    "    \n",
    "    idx_to_transfer = np.random.choice(range(len(list_one)), nb_items_to_transfer, replace=False)\n",
    "    content_to_transfer = [e for idx, e in enumerate(list_one) if idx in idx_to_transfer]\n",
    "    \n",
    "    for e in content_to_transfer:\n",
    "        list_one.remove(e)\n",
    "        list_two.append(e)\n",
    "    \n",
    "    return list_one, list_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_split(ds, target_idx=-1, random_state=42, extension='arff'):\n",
    "    \n",
    "    is_arff = extension == 'arff' # In arff, we do not encode. In csv, we do.\n",
    "    \n",
    "    # filenames\n",
    "    fn = original_filename(ds)\n",
    "    fn_train = filename_dataset(ds, step=1, suffix='train', extension=extension)\n",
    "    fn_test = filename_dataset(ds, step=1, suffix='test', extension=extension)\n",
    "    \n",
    "    # Loading\n",
    "    df, af = arff_to_df(fn, encode_nominal=not is_arff, return_af=True)    \n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=random_state, stratify=df.iloc[:, target_idx])\n",
    "    \n",
    "    if is_arff:\n",
    "        # Save train\n",
    "        af_train = af.copy()\n",
    "        af_train['data'] = train.values\n",
    "        with open(fn_train, 'w') as f:\n",
    "            arff.dump(af_train, f)\n",
    "\n",
    "        # Save test\n",
    "        af_test = af.copy()\n",
    "        af_test['data'] = test.values\n",
    "        with open(fn_test, 'w') as f:\n",
    "            arff.dump(af_test, f)\n",
    "    else:\n",
    "        # Save train\n",
    "        train.to_csv(fn_train, index=False, header=False)\n",
    "\n",
    "        # Save test\n",
    "        test.to_csv(fn_test, index=False, header=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def generate_queries(dataset, max_nb_queries=10, target_idx=-1, random_state=42):\n",
    "    q_codes = []\n",
    "    \n",
    "    # Derive Parameters\n",
    "    fn_test = filename_dataset(dataset, step=1, suffix='test')\n",
    "    df_test, af_test = arff_to_df(fn_test, encode_nominal=False, return_af=True)\n",
    "    \n",
    "    nb_atts = len(df_test.columns)\n",
    "    nb_qry = min(nb_atts-1, max_nb_queries)\n",
    "    \n",
    "    # Generate queries\n",
    "    q_desc, q_targ, q_miss = generate_query(nb_atts, targ_idx=-1, nb_qry=nb_qry, random_state=random_state)\n",
    "    \n",
    "    for q_idx in range(nb_qry):\n",
    "        q_codes.append(query_to_code(q_desc[q_idx], q_targ[q_idx], q_miss[q_idx]))\n",
    "        \n",
    "    q_codes = np.r_[q_codes] # Convert to proper np.ndarray\n",
    "    \n",
    "    # Save\n",
    "    fn_qry = filename_query(dataset, suffix=\"default\")\n",
    "    np.save(fn_qry, q_codes)\n",
    "    \n",
    "    return\n",
    "\n",
    "def generate_query_testsets(dataset, extension='arff'):\n",
    "    is_arff = extension == 'arff' # In arff, we do not encode. In csv, we do.\n",
    "    \n",
    "    # Load\n",
    "    fn_test = filename_dataset(dataset, step=1, suffix='test', extension=extension)\n",
    "    fn_q_codes = filename_query(dataset, suffix=\"default\")\n",
    "    \n",
    "    q_codes = np.load(fn_q_codes)\n",
    "    \n",
    "    if is_arff:\n",
    "        df_test, af_test = arff_to_df(fn_test, encode_nominal=False, return_af=True)\n",
    "    else:\n",
    "        df_test = pd.read_csv(fn_test, header=None, index_col=None)\n",
    "        #print(df.head())\n",
    "    \n",
    "    for q_idx, q_code in enumerate(q_codes):\n",
    "        _, _, q_miss = code_to_query(q_code)\n",
    "        fn_qry = filename_dataset(dataset, step=2, suffix='q_{}'.format(str(q_idx).zfill(3)), extension=extension)\n",
    "        \n",
    "        df_qry = df_test.copy()\n",
    "        df_qry.iloc[:, q_miss] = np.nan\n",
    "        \n",
    "        if is_arff:\n",
    "            af_qry = af_test.copy()\n",
    "            af_qry['data'] = df_qry.values\n",
    "            with open(fn_qry, 'w') as f:\n",
    "                arff.dump(af_qry, f)\n",
    "        else:\n",
    "            df_qry.to_csv(fn_qry, index=False, header=False)\n",
    "    return\n",
    "\n",
    "def check_nan(dataset):\n",
    "    fn = original_filename(dataset)\n",
    "    print(dataset)\n",
    "    \n",
    "    df, af = arff_to_df(fn, encode_nominal=False, return_af=True)\n",
    "    \n",
    "    if df.isnull().any().any():\n",
    "        msg = \"\"\"\n",
    "        Dataset: {} has NaN values from the start. We don't deal with this well in sklearn yet.\n",
    "        \"\"\".format(dataset)\n",
    "        print(msg)\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Work\n",
    "\n",
    "Here I implement the actual workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "\n",
      "    Starting dataset glass.\n",
      "    \n",
      "\n",
      "    Starting dataset credit-g.\n",
      "    \n",
      "\n",
      "    Starting dataset ionosphere.\n",
      "    \n",
      "\n",
      "    Starting dataset lymph.\n",
      "    \n",
      "\n",
      "    Starting dataset vehicle.\n",
      "    \n",
      "\n",
      "    Starting dataset iris.\n",
      "    \n",
      "\n",
      "    Starting dataset splice.\n",
      "    \n",
      "\n",
      "    Starting dataset sonar.\n",
      "    \n",
      "\n",
      "    Starting dataset vowel.\n",
      "    \n",
      "\n",
      "    Starting dataset segment.\n",
      "    \n",
      "\n",
      "    Starting dataset zoo.\n",
      "    \n",
      "\n",
      "    Starting dataset heart-statlog.\n",
      "    \n",
      "\n",
      "    Starting dataset waveform-5000.\n",
      "    \n",
      "\n",
      "    Starting dataset kr-vs-kp.\n",
      "    \n",
      "\n",
      "    Starting dataset diabetes.\n",
      "    \n",
      "\n",
      "    Starting dataset letter.\n",
      "    \n",
      "\n",
      "    Starting dataset balance-scale.\n",
      "    \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "datasets = ['glass',\n",
    "             'credit-g',\n",
    "             'ionosphere',\n",
    "             'lymph',\n",
    "             'vehicle',\n",
    "             'iris',\n",
    "             'splice',\n",
    "             'sonar',\n",
    "             'vowel',\n",
    "             'segment',\n",
    "             'zoo',\n",
    "             'heart-statlog',\n",
    "             'waveform-5000',\n",
    "             'kr-vs-kp',\n",
    "             'diabetes',\n",
    "             'letter',\n",
    "             'balance-scale']\n",
    "\n",
    "print(len(datasets))\n",
    "\n",
    "for ds in datasets:\n",
    "    msg = \"\"\"\n",
    "    Starting dataset {}.\n",
    "    \"\"\".format(ds)\n",
    "    print(msg)\n",
    "    \n",
    "    generate_train_test_split(ds, random_state=RANDOM_STATE, extension='arff')\n",
    "    \n",
    "    generate_queries(ds,random_state=RANDOM_STATE)\n",
    "    \n",
    "    generate_query_testsets(ds, extension='arff')\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Starting csv dataset glass.\n",
      "    \n",
      "\n",
      "    Starting csv dataset credit-g.\n",
      "    \n",
      "\n",
      "    Starting csv dataset ionosphere.\n",
      "    \n",
      "\n",
      "    Starting csv dataset lymph.\n",
      "    \n",
      "\n",
      "    Starting csv dataset vehicle.\n",
      "    \n",
      "\n",
      "    Starting csv dataset iris.\n",
      "    \n",
      "\n",
      "    Starting csv dataset splice.\n",
      "    \n",
      "\n",
      "    Starting csv dataset sonar.\n",
      "    \n",
      "\n",
      "    Starting csv dataset vowel.\n",
      "    \n",
      "\n",
      "    Starting csv dataset segment.\n",
      "    \n",
      "\n",
      "    Starting csv dataset zoo.\n",
      "    \n",
      "\n",
      "    Starting csv dataset heart-statlog.\n",
      "    \n",
      "\n",
      "    Starting csv dataset waveform-5000.\n",
      "    \n",
      "\n",
      "    Starting csv dataset kr-vs-kp.\n",
      "    \n",
      "\n",
      "    Starting csv dataset diabetes.\n",
      "    \n",
      "\n",
      "    Starting csv dataset letter.\n",
      "    \n",
      "\n",
      "    Starting csv dataset balance-scale.\n",
      "    \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets:\n",
    "    msg = \"\"\"\n",
    "    Starting csv dataset {}.\n",
    "    \"\"\".format(ds)\n",
    "    print(msg)\n",
    "    \n",
    "    generate_train_test_split(ds, random_state=RANDOM_STATE, extension='csv')\n",
    "    \n",
    "    generate_queries(ds,random_state=RANDOM_STATE)\n",
    "    \n",
    "    generate_query_testsets(ds, extension='csv')\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Which datasets can we conceivably use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [os.path.splitext(f) for f in os.listdir('../data/raw/datasets-UCI/UCI/')]\n",
    "datasets = [f for f, e in datasets if f not in {'breast-w'}]\n",
    "datasets = [ds for ds in datasets if check_nan(ds)]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaai20-belafonte",
   "language": "python",
   "name": "aaai20-belafonte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
